# 创建删除文件系统

## 创建文件系统

在试用CephFS，之前需要创建对应的文件系统，可以参考如下步骤：

1 检查集群状态，确保集群状态正常

```
# ceph health
```

2 确保MDS进程启动，如果没有部署MDS，则需要按照以下方法进行部署。一般建议把MDS服务部署到单独的服务器上。

```
2.1 为mds实例创建一个keyring
# ceph-authtool --create-keyring /etc/ceph/ceph.client.mds.keyring
# chmod +r /etc/ceph/ceph.client.mds.keyring
 
2.2 为mds实例创建一个用户和key
# ceph-authtool /etc/ceph/ceph.client.mds.keyring -n mds.server-70 --gen-key
 
2.3为这个key分配权限
# ceph-authtool -n mds.server-70 --cap osd 'allow rwx' --cap mon 'allow rwx' --cap mds 'allow rwx' /etc/ceph/ceph.client.mds.keyring
 
2.4 一旦创建了key，并且为key分配了访问cluster的权限，则将key加入到Ceph集群中。
# ceph -k /etc/ceph/ceph.client.admin.keyring auth add mds.server-70 -i /etc/ceph/ceph.client.mds.keyring
added key for client.server-70
 
2.5 编辑conf文件
# vim /etc/ceph/ceph.conf
[mds.server-70]
host = server-70
mds_data = /var/lib/ceph/mds/server-70/
keyring = /etc/ceph/ceph.client.mds.keyring
 
2.6 启动mds服务
# service ceph start mds
=== mds.server-70 ===
Starting Ceph mds.server-70 on server-70...
Running as unit run-6924.service.
# ps -ef|grep mds
root       6925      1  0 08:54 ?        00:00:00 /bin/bash -c ulimit -n 32768; /usr/bin/ceph-mds -i server-70 --pid-file /var/run/ceph/mds.server-70 -c /etc/ceph/ceph.conf --cluster ceph -f
root       6929   6925  3 08:54 ?        00:00:00 /usr/bin/ceph-mds -i server-70 --pid-file /var/run/ceph/mds.server-70 -c /etc/ceph/ceph.conf --cluster ceph -f
 
2.7 确认mds的状态为up:active
# ceph mds stat
e21: 1/1/0 up {0=ceph-03=up:active}
```

如果要部署多个MDS，可以参照上面的步骤完成，默认情况下，添加多个MDS时，自动采用Active－Standby模式，只有一个MDS的状态为Active，剩余其他的均为Standby模式。

3. 创建一个CephFS，目前一个集群只支持创建一个文件系统。

```
# ceph osd pool create {datapoolname} {pg-num} {pgp-num}
# ceph osd pool set {datapoolname} crush_ruleset {rule-set}
# ceph osd pool set {datapoolname} size {replica-size}
# ceph osd pool create {metadatapoolname} {pg-num} {pgp-num}
# ceph osd pool set {metadatapoolname} crush_ruleset {rule-set}
# ceph osd pool set {metadatapoolname} size {replica-size}
# ceph fs new {cephfs-name} {metadatapoolname} {datapoolname}
```

把其中{datapoolname}、｛metadatapoolname}、｛cephfs-name｝替换成对应的名字即可，然后设置对应的pg\/pgp个数，以及副本数即可。确保文件系统已经创建成功：

```
# ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
```

## 删除文件系统

如果想删除掉文件系统，可以参照以下步骤完成：

* 确保没有程序还在访问cephfs，如果有先停止对CephFS的访问。
* 停掉所有MDS进程

```
# service ceph stop mds
```

* 设置MDS的状态为fail

```
# ceph mds fail {mds-id}
```

其中mds-id，可以从' ceph mds stat ‘ 的输出中找出，比如下面的例子，MDS ceph-03的mds-id为0

```
# ceph mds stat
e28: 1/1/1 up {0=ceph-03=up:active}
```

* 删除CephFS

```
# ceph fs rm {cephfs-name} --yes-i-really-mean-it
```

如果想彻底删除掉CephFS，则需要把之前创建的两个池子也删除掉。

```
# ceph osd pool delete {datapoolname} {datapoolname} --yes-i-really-really-mean-it
# ceph osd pool delete {metadatapoolname} {metadatapoolname} --yes-i-really-really-mean-it
```


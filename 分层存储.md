# 分层存储

## 1 简介

基本思想是冷热数据分层，使用高性能的存储设备比如SSD构建一个Cache Pool存放热数据，而使用廉价大容量的存储设备如SATA\/SAS等构建一个Backing Pool存放不经常访问的冷数据，如下图所示。上层应用程序的IO都是和Cache Pool打交道，当Cache Pool中的容量不足时，会按照一定策略将数据回刷到Backing Pool中。而当应用程序要读取的数据不在Cache Pool中，数据会被从Backing Pool中拉回到Cache Pool中。

![](/assets/D4EB1442-DC24-4F70-9F8B-F7243B75F7DF.png)

* 每一个缓存层次（tiered cache）使用一个 RADOS pool，其中 cache pool 必须是拷贝（replicated）类型，而backing pool 可以是拷贝类型也可以是纠错码类型。
* 在不同的缓存层次，使用不同的硬件介质，cache pool 使用的介质必须比 backing pool 使用的介质速度快：比如，在 backing pool 使用一般的存储介质，比如常规的HDD或者 SATA SDD；在 cache pool 使用快速介质，比如PCIe SDD。
* 每一个 tiered cache 使用自己的 CRUSH rules，使得数据会被写入到指定的不同存储介质。
* librados 内在支持 tiered cache，大多数情况下它会知道客户端数据需要被放到哪一层，因此不需要在RDB，CephFS，RGW 客户端上做改动。
* OSD 独立地处理数据在两个层次之间的流动：promotion（HDD-&gt;SDD）和 eviction（SDD -&gt; HDD），但是，这种数据流动是代价昂贵（expensive）和耗时的（take long time to “warm up”）

![](/assets/DD8AC1D1-221F-4756-9CE2-B4B1C5E5EC58.png)
Ceph Cache Tiering Agent处理缓存层和存储层的数据的自动迁移，对客户端透明操作透明。Cahe层有两种典型使用模式：

1）writeback模式

Ceph客户端直接往Cache层写数据，写完立即返回，Agent再及时把数据迁移到冷数据池。当客户端取不在Cache层的冷数据 时，Agent负责把冷数据迁移到Cache层。也就是说，Ceph客户端直接在Cache层上进行IO读写操作，不会与相对慢速的冷数据池进行数据交 换。

这种模式适用于可变数据的操作，如照片\/视频编辑、电商交易数据等等。

2）只读模式

Ceph客户端在写操作时往后端冷数据池直接写，读数据时，Ceph把数据从后端读取到Cache层。

这种模式适用于不可变数据，如微博\/微信上的照片\/视频、DNA数据、X射线影像等。

## 2 配置使用

下面是ceph cache tiering的部署步骤

1）创建cache tiering，分为以下几步

* 创建好两个pool，基于高速设备的为cachepool，基于低速设备的为storagepool
* 添加tier
* 设置cache tier的模式
* 激活cache tier的设置

具体命令如下所示：

```
ceph osd tier add {storagepool} {cachepool}
ceph osd tier cache-mode {cachepool} {cache-mode}
ceph osd tier set-overlay {storagepool} {cachepool}
```

2）设置相关参数

参数设置主要是设置hit set以及数据从cachepool到storagepool的回刷条件。Ceph通过hit set来实现数据缓存策略，hit set会记录cache tier中object 的访问热度，能够把经常访问的object驻留在cachepool中。

Cache tier目前支持以下配置参数，

* hit\_set\_type，Ceph中目前只支持bloom filter类型的的hit set。

* hit\_set\_count，定义了每个hit set覆盖的时间区间。

* hit\_set\_period，定义了所有hit set保留多长时间的数据。

* target\_max\_bytes，当cache的脏数据量达到这个设置之后，会触发数据的回刷。

* target\_max\_objects，当cache的脏的objects达到这个设置之后，会触发数据的回刷。

* cache\_target\_dirty\_ratio，当cache的脏数据比率达到该设置之后，cache tier agent会触发数据的回刷。

* cache\_target\_full\_ratio，当cache的脏数据比率达到该设置之后，cache tier agent会加速数据的回刷。
* cache\_min\_flush\_age，设置cache tier agent延迟多久才将缓存中的脏数据回刷到storagepool。
* cache\_min\_evict\_age，设置对象在缓存中至少停留的时间，必须过了该时间设置之后，才能将其从缓存中逐出。

可以通过以下方法进行配置。

```
# ceph osd pool set {cachepool} {key} {value}
```

下面给出一个常用的配置。

```
ceph osd pool set {cachepool} hit_set_type bloom
ceph osd pool set {cachepool} hit_set_count 24
ceph osd pool set {cachepool} hit_set_period 3600
ceph osd pool set {cachepool} target_max_bytes 375809638400
ceph osd pool set {cachepool} target_max_objects 1000000
ceph osd pool set {cachepool} cache_target_dirty_ratio 0.4
ceph osd pool set {cachepool} cache_target_full_ratio 0.8
ceph osd pool set {cachepool} cache_min_flush_age 600
ceph osd pool set {cachepool} cache_min_evict_age 1800
```

3）删除cache tiering

```
ceph osd tier cache-mode {cachepool} forward
rados -p {cachepool} ls
rados -p {cachepool} cache-flush-evict-all
ceph osd tier remove-overlay {storagetier}
ceph osd tier remove {storagepool} {cachepool}
```

## 2 cache tier的应用

### **2.1 cache tier在块存储中的应用**

如果需要测试rbd的性能，

1）基于高速设备（通常是SSD）创建pool，如cachepool

2）基于低速设备（通常是SATA）创建pool，如storagepool

3）通过脚本，配置cahce tiering，如下：

```
$ cat create-tier.sh
#!/bin/sh
cachepool=$1
storagepool=$2
ceph osd tier add $storagepool $cachepool
ceph osd tier cache-mode $cachepool writeback
ceph osd tier set-overlay $storagepool $cachepool
ceph osd pool set $cachepool hit_set_type bloom
ceph osd pool set $cachepool hit_set_count 24
ceph osd pool set $cachepool hit_set_period 3600
ceph osd pool set $cachepool target_max_bytes 375809638400    
ceph osd pool set $cachepool target_max_objects 1000000
ceph osd pool set $cachepool cache_target_dirty_ratio 0.4
ceph osd pool set $cachepool cache_target_full_ratio 0.8
ceph osd pool set {cachepool} cache_min_flush_age 600
ceph osd pool set {cachepool} cache_min_evict_age 1800
```

4）基于storagepool创建block，然后使用。

5）测试方法

*  创建多个volume（7~8个）：

 rbd create storagepool\/test --size 102400 --image-format 2

 rbd create storagepool\/test1 --size 102400 --image-format 2

 ...

 rbd create storagepool\/test7 --size 102400 --image-format 2

*  选择其中一台物理节点，对每个创建的volume，使用rbd map 映射到本地

 rbd map storagepool\/test

 ...

 rbd map storagepool\/test7

*  使用fio压力测试，将以下信息修改并保存为cache\_tiering.fio，并执行fio cache\_tiering.fio：\(如果可以创建虚拟机，也可以创建多台虚拟机进行测试，每台虚拟机使用4k随机写的方式进行测试\)

 **这个脚本仅限于测试IOPS**

```
[global]
iodepth=64
bs=4k
direct=1
rw=randwrite
ioengine=libaio
fdatasync=1
thread
numjobs=1


[job1]
filename=/dev/rbd0
runtime=60
name=ebs1
[job2]
filename=/dev/rbd1
runtime=60
name=ebs2

[job3]
filename=/dev/rbd2
runtime=60
name=ebs3
[job4]
filename=/dev/rbd3
runtime=60
name=ebs4
....                      #此处需要增加job
[job8]
filename=/dev/rbd7
runtime=60
name=ebs5
```

 如果测试吞吐，将bs改为1M; 如果测试延时，将bs改为4k，iodepth改为1；

### **2.1 cache tier在CephFS下的使用配置**

1）搭建Ceph FS的环境，假设Cephfs使用的pool是metadata和data

2）基于PCIE SSD，创建两个Cache Pool，分别为cachemetadata和cachedata

3）配置cahce tiering，如下：

```
$ cat create-tier.sh
#!/bin/sh
cachepool=$1
storagepool=$2
ceph osd tier add $storagepool $cachepool
ceph osd tier cache-mode $cachepool writeback
ceph osd tier set-overlay $storagepool $cachepool
ceph osd pool set $cachepool hit_set_type bloom
ceph osd pool set $cachepool hit_set_count 24
ceph osd pool set $cachepool hit_set_period 3600
ceph osd pool set $cachepool target_max_bytes 375809638400
ceph osd pool set $cachepool target_max_objects 1000000
ceph osd pool set $cachepool cache_target_dirty_ratio 0.4
ceph osd pool set $cachepool cache_target_full_ratio 0.8
ceph osd pool set {cachepool} cache_min_flush_age 600
ceph osd pool set {cachepool} cache_min_evict_age 1800
```

可以通过以上脚本，创建两个tier，分别执行：

```
$ ./create-tier.sh cachemetadata metadata
$ ./create-tier.sh cachedata data 
```

### **2.2 cache tier在RGW对象存储下的使用配置：**

1）搭建Ceph 对象存储的环境，因为rgw需要使用很多个pool，但其中和性能相关的主要有两个.rgw.buckets和.rgw.buckets.index，所以只需要为这两个pool建立tier即可。

2）基于PCIE SSD，创建两个Cache Pool，分别为cachebucketindex和cachebucket

3）配置cahce tiering，如下：

```
$ cat create-tier.sh
#!/bin/sh
cachepool=$1
storagepool=$2
ceph osd tier add $storagepool $cachepool
ceph osd tier cache-mode $cachepool writeback
ceph osd tier set-overlay $storagepool $cachepool
ceph osd pool set $cachepool hit_set_type bloom
ceph osd pool set $cachepool hit_set_count 24
ceph osd pool set $cachepool hit_set_period 3600
ceph osd pool set $cachepool target_max_bytes 375809638400
ceph osd pool set $cachepool target_max_objects 1000000
ceph osd pool set $cachepool cache_target_dirty_ratio 0.4
ceph osd pool set $cachepool cache_target_full_ratio 0.8
ceph osd pool set {cachepool} cache_min_flush_age 600
ceph osd pool set {cachepool} cache_min_evict_age 1800
```

可以通过以上脚本，创建两个tier，分别执行：

```
$ ./create-tier.sh cachebucketindex .rgw.buckets.index
$ ./create-tier.sh cachebucket .rgw.buckets 
```

